{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - Delta Live Tables Pipeline with SFTP\n",
    "\n",
    "This notebook implements a complete DLT pipeline that:\n",
    "1. Reads data from source SFTP using AutoLoader\n",
    "2. Processes and transforms data through bronze → silver → gold layers\n",
    "3. Writes processed data back to target SFTP using custom SFTP data source\n",
    "\n",
    "**Note:** This notebook should be run as a DLT pipeline in Databricks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import dlt\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.types import *\nfrom ingest import SFTPDataSource"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration from config table\n",
    "config_df = spark.table(\"sftp_demo.config.connection_params\")\n",
    "config_dict = {row.key: row.value for row in config_df.collect()}\n",
    "\n",
    "# Source SFTP configuration\n",
    "SOURCE_HOST = config_dict[\"source_host\"]\n",
    "SOURCE_USERNAME = config_dict[\"source_username\"]\n",
    "SOURCE_CONNECTION = \"sftp_demo.source_sftp_connection\"\n",
    "\n",
    "# Target SFTP configuration\n",
    "TARGET_HOST = config_dict[\"target_host\"]\n",
    "TARGET_USERNAME = config_dict[\"target_username\"]\n",
    "SSH_KEY_PATH = config_dict[\"ssh_key_path\"]\n",
    "\n",
    "target_sftp_config = {\n",
    "    \"host\": TARGET_HOST,\n",
    "    \"username\": TARGET_USERNAME,\n",
    "    \"private_key_path\": SSH_KEY_PATH,\n",
    "    \"port\": 22\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bronze Layer: Raw Data Ingestion\n",
    "\n",
    "Use AutoLoader to read CSV files from source SFTP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dlt.table(\n",
    "    name=\"bronze_customers\",\n",
    "    comment=\"Raw customer data from source SFTP\",\n",
    "    table_properties={\"quality\": \"bronze\"}\n",
    ")\n",
    "def bronze_customers():\n",
    "    \"\"\"Ingest raw customer data from SFTP using AutoLoader\"\"\"\n",
    "    return (\n",
    "        spark.readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"csv\")\n",
    "        .option(\"cloudFiles.connectionName\", SOURCE_CONNECTION)\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"inferSchema\", \"true\")\n",
    "        .load(f\"sftp://{SOURCE_HOST}/customers.csv\")\n",
    "        .withColumn(\"ingestion_timestamp\", F.current_timestamp())\n",
    "        .withColumn(\"source_file\", F.input_file_name())\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dlt.table(\n",
    "    name=\"bronze_orders\",\n",
    "    comment=\"Raw order data from source SFTP\",\n",
    "    table_properties={\"quality\": \"bronze\"}\n",
    ")\n",
    "def bronze_orders():\n",
    "    \"\"\"Ingest raw order data from SFTP using AutoLoader\"\"\"\n",
    "    return (\n",
    "        spark.readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"csv\")\n",
    "        .option(\"cloudFiles.connectionName\", SOURCE_CONNECTION)\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"inferSchema\", \"true\")\n",
    "        .load(f\"sftp://{SOURCE_HOST}/orders.csv\")\n",
    "        .withColumn(\"ingestion_timestamp\", F.current_timestamp())\n",
    "        .withColumn(\"source_file\", F.input_file_name())\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Silver Layer: Cleaned and Validated Data\n",
    "\n",
    "Apply data quality rules and transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dlt.table(\n",
    "    name=\"silver_customers\",\n",
    "    comment=\"Cleaned and validated customer data\",\n",
    "    table_properties={\"quality\": \"silver\"}\n",
    ")\n",
    "@dlt.expect_or_drop(\"valid_customer_id\", \"customer_id IS NOT NULL\")\n",
    "@dlt.expect_or_drop(\"valid_email\", \"email IS NOT NULL AND email LIKE '%@%'\")\n",
    "@dlt.expect(\"valid_signup_date\", \"signup_date IS NOT NULL\")\n",
    "def silver_customers():\n",
    "    \"\"\"Clean and validate customer data\"\"\"\n",
    "    return (\n",
    "        dlt.read_stream(\"bronze_customers\")\n",
    "        .select(\n",
    "            \"customer_id\",\n",
    "            F.trim(F.col(\"name\")).alias(\"name\"),\n",
    "            F.lower(F.trim(F.col(\"email\"))).alias(\"email\"),\n",
    "            F.upper(F.trim(F.col(\"country\"))).alias(\"country\"),\n",
    "            F.to_date(F.col(\"signup_date\")).alias(\"signup_date\"),\n",
    "            F.current_timestamp().alias(\"processed_timestamp\")\n",
    "        )\n",
    "        .dropDuplicates([\"customer_id\"])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dlt.table(\n",
    "    name=\"silver_orders\",\n",
    "    comment=\"Cleaned and validated order data\",\n",
    "    table_properties={\"quality\": \"silver\"}\n",
    ")\n",
    "@dlt.expect_or_drop(\"valid_order_id\", \"order_id IS NOT NULL\")\n",
    "@dlt.expect_or_drop(\"valid_customer_id\", \"customer_id IS NOT NULL\")\n",
    "@dlt.expect_or_drop(\"valid_amount\", \"amount > 0\")\n",
    "@dlt.expect_or_drop(\"valid_quantity\", \"quantity > 0\")\n",
    "def silver_orders():\n",
    "    \"\"\"Clean and validate order data\"\"\"\n",
    "    return (\n",
    "        dlt.read_stream(\"bronze_orders\")\n",
    "        .select(\n",
    "            \"order_id\",\n",
    "            \"customer_id\",\n",
    "            F.trim(F.col(\"product\")).alias(\"product\"),\n",
    "            F.col(\"quantity\").cast(\"int\").alias(\"quantity\"),\n",
    "            F.col(\"amount\").cast(\"decimal(10,2)\").alias(\"amount\"),\n",
    "            F.to_date(F.col(\"order_date\")).alias(\"order_date\"),\n",
    "            F.current_timestamp().alias(\"processed_timestamp\")\n",
    "        )\n",
    "        .dropDuplicates([\"order_id\"])\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gold Layer: Business-Level Aggregations\n",
    "\n",
    "Create enriched datasets for analytics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dlt.table(\n",
    "    name=\"gold_customer_orders\",\n",
    "    comment=\"Enriched customer order data with aggregations\",\n",
    "    table_properties={\"quality\": \"gold\"}\n",
    ")\n",
    "def gold_customer_orders():\n",
    "    \"\"\"Create enriched customer order dataset\"\"\"\n",
    "    customers = dlt.read_stream(\"silver_customers\")\n",
    "    orders = dlt.read_stream(\"silver_orders\")\n",
    "    \n",
    "    return (\n",
    "        orders\n",
    "        .join(customers, \"customer_id\", \"left\")\n",
    "        .select(\n",
    "            \"order_id\",\n",
    "            \"customer_id\",\n",
    "            \"name\",\n",
    "            \"email\",\n",
    "            \"country\",\n",
    "            \"product\",\n",
    "            \"quantity\",\n",
    "            \"amount\",\n",
    "            \"order_date\",\n",
    "            F.current_timestamp().alias(\"processed_timestamp\")\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dlt.table(\n",
    "    name=\"gold_customer_summary\",\n",
    "    comment=\"Customer summary metrics\",\n",
    "    table_properties={\"quality\": \"gold\"}\n",
    ")\n",
    "def gold_customer_summary():\n",
    "    \"\"\"Calculate customer summary metrics\"\"\"\n",
    "    return (\n",
    "        dlt.read_stream(\"gold_customer_orders\")\n",
    "        .groupBy(\"customer_id\", \"name\", \"email\", \"country\")\n",
    "        .agg(\n",
    "            F.count(\"order_id\").alias(\"total_orders\"),\n",
    "            F.sum(\"amount\").alias(\"total_amount\"),\n",
    "            F.avg(\"amount\").alias(\"avg_order_amount\"),\n",
    "            F.min(\"order_date\").alias(\"first_order_date\"),\n",
    "            F.max(\"order_date\").alias(\"last_order_date\")\n",
    "        )\n",
    "        .withColumn(\"processed_timestamp\", F.current_timestamp())\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write to Target SFTP\n",
    "\n",
    "Use custom SFTP data source to write processed data back to target SFTP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_sftp(df, remote_path):\n",
    "    \"\"\"\n",
    "    Write DataFrame to target SFTP using custom data source\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to write\n",
    "        remote_path: Remote file path on SFTP\n",
    "    \"\"\"\n",
    "    # Convert to Pandas for writing (for batch processing)\n",
    "    pdf = df.toPandas()\n",
    "    \n",
    "    # Write to SFTP\n",
    "    writer = SFTPDataSource.create_writer(target_sftp_config)\n",
    "    with writer.session():\n",
    "        writer.write_dataframe(\n",
    "            pdf,\n",
    "            remote_path,\n",
    "            format=\"csv\",\n",
    "            header=True\n",
    "        )\n",
    "    \n",
    "    print(f\"Data written to SFTP: {remote_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dlt.table(\n",
    "    name=\"target_customer_summary\",\n",
    "    comment=\"Customer summary data for SFTP export\",\n",
    "    table_properties={\"quality\": \"gold\"}\n",
    ")\n",
    "def target_customer_summary():\n",
    "    \"\"\"Prepare customer summary for SFTP export\"\"\"\n",
    "    return dlt.read(\"gold_customer_summary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export gold layer data to target SFTP\n",
    "# This would typically be triggered after the DLT pipeline completes\n",
    "\n",
    "def export_to_sftp():\n",
    "    \"\"\"\n",
    "    Export processed data to target SFTP\n",
    "    Run this after DLT pipeline completes\n",
    "    \"\"\"\n",
    "    # Read gold layer data\n",
    "    customer_summary_df = spark.table(\"sftp_demo.gold.gold_customer_summary\")\n",
    "    customer_orders_df = spark.table(\"sftp_demo.gold.gold_customer_orders\")\n",
    "    \n",
    "    # Write to target SFTP\n",
    "    write_to_sftp(customer_summary_df, \"/customer_summary.csv\")\n",
    "    write_to_sftp(customer_orders_df, \"/customer_orders.csv\")\n",
    "    \n",
    "    print(\"Export to target SFTP completed successfully\")\n",
    "\n",
    "# Uncomment to run export after pipeline\n",
    "# export_to_sftp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verification: Read from Target SFTP\n",
    "\n",
    "Verify that data was successfully written to target SFTP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_target_sftp():\n",
    "    \"\"\"\n",
    "    Verify data in target SFTP\n",
    "    \"\"\"\n",
    "    writer = SFTPDataSource.create_writer(target_sftp_config)\n",
    "    \n",
    "    with writer.session():\n",
    "        files = writer.list_files(\".\")\n",
    "        print(\"Files in target SFTP:\")\n",
    "        for f in files:\n",
    "            print(f\"  - {f}\")\n",
    "    \n",
    "    # Read back using AutoLoader to verify\n",
    "    target_connection = \"sftp_demo.target_sftp_connection\"\n",
    "    \n",
    "    verification_df = (\n",
    "        spark.read\n",
    "        .format(\"csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"inferSchema\", \"true\")\n",
    "        .load(f\"sftp://{TARGET_HOST}/customer_summary.csv\")\n",
    "    )\n",
    "    \n",
    "    print(\"\\nSample data from target SFTP:\")\n",
    "    verification_df.show(10)\n",
    "    \n",
    "    print(f\"\\nTotal records in target: {verification_df.count()}\")\n",
    "\n",
    "# Uncomment to run verification\n",
    "# verify_target_sftp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Execution Summary\n",
    "\n",
    "To run this complete pipeline:\n",
    "\n",
    "1. **Create DLT Pipeline in Databricks:**\n",
    "   - Go to Delta Live Tables → Create Pipeline\n",
    "   - Set notebook path to this notebook\n",
    "   - Configure target catalog: `sftp_demo`\n",
    "   - Set storage location for checkpoints\n",
    "   - Enable AutoLoader for continuous ingestion\n",
    "\n",
    "2. **Pipeline Configuration:**\n",
    "   ```json\n",
    "   {\n",
    "     \"name\": \"SFTP Data Pipeline\",\n",
    "     \"storage\": \"/dbfs/sftp_demo/dlt\",\n",
    "     \"target\": \"sftp_demo\",\n",
    "     \"continuous\": false,\n",
    "     \"development\": true\n",
    "   }\n",
    "   ```\n",
    "\n",
    "3. **Start Pipeline:**\n",
    "   - Click \"Start\" to run the pipeline\n",
    "   - Monitor data flow: Bronze → Silver → Gold\n",
    "   - Check data quality metrics\n",
    "\n",
    "4. **Export to Target SFTP:**\n",
    "   - After pipeline completes, run `export_to_sftp()`\n",
    "   - Verify with `verify_target_sftp()`\n",
    "\n",
    "### Data Flow:\n",
    "```\n",
    "Source SFTP (customers.csv, orders.csv)\n",
    "    ↓ AutoLoader\n",
    "Bronze Layer (raw data)\n",
    "    ↓ Cleaning & Validation\n",
    "Silver Layer (validated data)\n",
    "    ↓ Enrichment & Aggregation\n",
    "Gold Layer (business metrics)\n",
    "    ↓ Custom SFTP Data Source\n",
    "Target SFTP (customer_summary.csv, customer_orders.csv)\n",
    "```\n",
    "\n",
    "### Success Criteria:\n",
    "- ✓ Bronze tables contain all source records\n",
    "- ✓ Silver tables have clean, validated data\n",
    "- ✓ Gold tables contain enriched business metrics\n",
    "- ✓ Data quality expectations passed\n",
    "- ✓ Target SFTP contains exported files\n",
    "- ✓ Data lineage visible in DLT UI"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}