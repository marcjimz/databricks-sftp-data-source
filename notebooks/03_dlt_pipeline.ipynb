{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 03 - Delta Live Tables Pipeline with SFTP\n\nThis notebook implements a complete DLT pipeline that:\n1. Reads data from source SFTP using AutoLoader\n2. Processes and transforms data through bronze → silver → gold layers\n3. Writes processed data back to target SFTP using custom SFTP data source\n\n**Note:** This notebook should be run as a DLT pipeline in Databricks. The custom SFTP package is imported directly from the src folder."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nimport os\nimport tempfile\nimport dlt\nfrom pyspark.sql import functions as F\n\n# Add src folder to Python path\nnotebook_path = os.path.dirname(dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get())\nrepo_root = os.path.dirname(notebook_path)\nsrc_path = os.path.join(repo_root, 'src')\n\nif src_path not in sys.path:\n    sys.path.insert(0, src_path)\n\n# Import custom SFTP package\nfrom ingest import SFTPDataSource"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Configuration Widgets\n\nSet the catalog and schema names. These should match the values used in notebooks 01 and 02.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Create widgets for catalog and schema configuration\ndbutils.widgets.text(\"catalog_name\", \"sftp_demo\", \"Catalog Name\")\ndbutils.widgets.text(\"schema_name\", \"default\", \"Schema Name\")\ndbutils.widgets.text(\"source_connection_name\", \"source_sftp_connection\", \"Source Connection Name\")\ndbutils.widgets.text(\"target_connection_name\", \"target_sftp_connection\", \"Target Connection Name\")\n\n# Get widget values\nCATALOG_NAME = dbutils.widgets.get(\"catalog_name\")\nSCHEMA_NAME = dbutils.widgets.get(\"schema_name\")\nSOURCE_CONNECTION_NAME = dbutils.widgets.get(\"source_connection_name\")\nTARGET_CONNECTION_NAME = dbutils.widgets.get(\"target_connection_name\")\n\nprint(f\"Catalog: {CATALOG_NAME}\")\nprint(f\"Schema: {SCHEMA_NAME}\")\nprint(f\"Source Connection: {SOURCE_CONNECTION_NAME}\")\nprint(f\"Target Connection: {TARGET_CONNECTION_NAME}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load configuration from config table\nconfig_df = spark.table(f\"{CATALOG_NAME}.config.connection_params\")\nconfig_dict = {row.key: row.value for row in config_df.collect()}\n\n# Source SFTP configuration\nSOURCE_HOST = config_dict[\"source_host\"]\nSOURCE_USERNAME = config_dict[\"source_username\"]\nSOURCE_CONNECTION = SOURCE_CONNECTION_NAME\n\n# Target SFTP configuration\nTARGET_HOST = config_dict[\"target_host\"]\nTARGET_USERNAME = config_dict[\"target_username\"]\nTARGET_CONNECTION = TARGET_CONNECTION_NAME\nSECRET_SCOPE = config_dict[\"secret_scope\"]\nSSH_KEY_SECRET = config_dict[\"ssh_key_secret\"]\n\n# Get SSH private key from secrets and write to temporary file\nssh_key_content = dbutils.secrets.get(scope=SECRET_SCOPE, key=SSH_KEY_SECRET)\ntmp_key_file = tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='_sftp_key')\ntmp_key_file.write(ssh_key_content)\ntmp_key_file.close()\nos.chmod(tmp_key_file.name, 0o600)\n\ntarget_sftp_config = {\n    \"host\": TARGET_HOST,\n    \"username\": TARGET_USERNAME,\n    \"private_key_path\": tmp_key_file.name,\n    \"port\": 22\n}\n\nprint(f\"Using catalog: {CATALOG_NAME}\")\nprint(f\"Using schema: {SCHEMA_NAME}\")\nprint(f\"Source connection: {SOURCE_CONNECTION}\")\nprint(f\"Target connection: {TARGET_CONNECTION}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bronze Layer: Raw Data Ingestion\n",
    "\n",
    "Use AutoLoader to read CSV files from source SFTP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "@dlt.table(\n    name=\"bronze_customers\",\n    comment=\"Raw customer data from source SFTP\",\n    table_properties={\"quality\": \"bronze\"}\n)\ndef bronze_customers():\n    \"\"\"Ingest raw customer data from SFTP using AutoLoader\"\"\"\n    source_uri = f\"sftp://{SOURCE_USERNAME}@{SOURCE_HOST}:22/customers.csv\"\n    \n    return (\n        spark.readStream\n        .format(\"cloudFiles\")\n        .option(\"cloudFiles.format\", \"csv\")\n        .option(\"cloudFiles.schemaLocation\", f\"/tmp/{CATALOG_NAME}/dlt/schema/customers\")\n        .option(\"header\", \"true\")\n        .load(source_uri)\n        .withColumn(\"ingestion_timestamp\", F.current_timestamp())\n        .withColumn(\"source_file\", F.input_file_name())\n    )"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "@dlt.table(\n    name=\"bronze_orders\",\n    comment=\"Raw order data from source SFTP\",\n    table_properties={\"quality\": \"bronze\"}\n)\ndef bronze_orders():\n    \"\"\"Ingest raw order data from SFTP using AutoLoader\"\"\"\n    source_uri = f\"sftp://{SOURCE_USERNAME}@{SOURCE_HOST}:22/orders.csv\"\n    \n    return (\n        spark.readStream\n        .format(\"cloudFiles\")\n        .option(\"cloudFiles.format\", \"csv\")\n        .option(\"cloudFiles.schemaLocation\", f\"/tmp/{CATALOG_NAME}/dlt/schema/orders\")\n        .option(\"header\", \"true\")\n        .load(source_uri)\n        .withColumn(\"ingestion_timestamp\", F.current_timestamp())\n        .withColumn(\"source_file\", F.input_file_name())\n    )"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Silver Layer: Cleaned and Validated Data\n",
    "\n",
    "Apply data quality rules and transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dlt.table(\n",
    "    name=\"silver_customers\",\n",
    "    comment=\"Cleaned and validated customer data\",\n",
    "    table_properties={\"quality\": \"silver\"}\n",
    ")\n",
    "@dlt.expect_or_drop(\"valid_customer_id\", \"customer_id IS NOT NULL\")\n",
    "@dlt.expect_or_drop(\"valid_email\", \"email IS NOT NULL AND email LIKE '%@%'\")\n",
    "@dlt.expect(\"valid_signup_date\", \"signup_date IS NOT NULL\")\n",
    "def silver_customers():\n",
    "    \"\"\"Clean and validate customer data\"\"\"\n",
    "    return (\n",
    "        dlt.read_stream(\"bronze_customers\")\n",
    "        .select(\n",
    "            \"customer_id\",\n",
    "            F.trim(F.col(\"name\")).alias(\"name\"),\n",
    "            F.lower(F.trim(F.col(\"email\"))).alias(\"email\"),\n",
    "            F.upper(F.trim(F.col(\"country\"))).alias(\"country\"),\n",
    "            F.to_date(F.col(\"signup_date\")).alias(\"signup_date\"),\n",
    "            F.current_timestamp().alias(\"processed_timestamp\")\n",
    "        )\n",
    "        .dropDuplicates([\"customer_id\"])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dlt.table(\n",
    "    name=\"silver_orders\",\n",
    "    comment=\"Cleaned and validated order data\",\n",
    "    table_properties={\"quality\": \"silver\"}\n",
    ")\n",
    "@dlt.expect_or_drop(\"valid_order_id\", \"order_id IS NOT NULL\")\n",
    "@dlt.expect_or_drop(\"valid_customer_id\", \"customer_id IS NOT NULL\")\n",
    "@dlt.expect_or_drop(\"valid_amount\", \"amount > 0\")\n",
    "@dlt.expect_or_drop(\"valid_quantity\", \"quantity > 0\")\n",
    "def silver_orders():\n",
    "    \"\"\"Clean and validate order data\"\"\"\n",
    "    return (\n",
    "        dlt.read_stream(\"bronze_orders\")\n",
    "        .select(\n",
    "            \"order_id\",\n",
    "            \"customer_id\",\n",
    "            F.trim(F.col(\"product\")).alias(\"product\"),\n",
    "            F.col(\"quantity\").cast(\"int\").alias(\"quantity\"),\n",
    "            F.col(\"amount\").cast(\"decimal(10,2)\").alias(\"amount\"),\n",
    "            F.to_date(F.col(\"order_date\")).alias(\"order_date\"),\n",
    "            F.current_timestamp().alias(\"processed_timestamp\")\n",
    "        )\n",
    "        .dropDuplicates([\"order_id\"])\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gold Layer: Business-Level Aggregations\n",
    "\n",
    "Create enriched datasets for analytics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dlt.table(\n",
    "    name=\"gold_customer_orders\",\n",
    "    comment=\"Enriched customer order data with aggregations\",\n",
    "    table_properties={\"quality\": \"gold\"}\n",
    ")\n",
    "def gold_customer_orders():\n",
    "    \"\"\"Create enriched customer order dataset\"\"\"\n",
    "    customers = dlt.read_stream(\"silver_customers\")\n",
    "    orders = dlt.read_stream(\"silver_orders\")\n",
    "    \n",
    "    return (\n",
    "        orders\n",
    "        .join(customers, \"customer_id\", \"left\")\n",
    "        .select(\n",
    "            \"order_id\",\n",
    "            \"customer_id\",\n",
    "            \"name\",\n",
    "            \"email\",\n",
    "            \"country\",\n",
    "            \"product\",\n",
    "            \"quantity\",\n",
    "            \"amount\",\n",
    "            \"order_date\",\n",
    "            F.current_timestamp().alias(\"processed_timestamp\")\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dlt.table(\n",
    "    name=\"gold_customer_summary\",\n",
    "    comment=\"Customer summary metrics\",\n",
    "    table_properties={\"quality\": \"gold\"}\n",
    ")\n",
    "def gold_customer_summary():\n",
    "    \"\"\"Calculate customer summary metrics\"\"\"\n",
    "    return (\n",
    "        dlt.read_stream(\"gold_customer_orders\")\n",
    "        .groupBy(\"customer_id\", \"name\", \"email\", \"country\")\n",
    "        .agg(\n",
    "            F.count(\"order_id\").alias(\"total_orders\"),\n",
    "            F.sum(\"amount\").alias(\"total_amount\"),\n",
    "            F.avg(\"amount\").alias(\"avg_order_amount\"),\n",
    "            F.min(\"order_date\").alias(\"first_order_date\"),\n",
    "            F.max(\"order_date\").alias(\"last_order_date\")\n",
    "        )\n",
    "        .withColumn(\"processed_timestamp\", F.current_timestamp())\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write to Target SFTP\n",
    "\n",
    "Use custom SFTP data source to write processed data back to target SFTP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_sftp(df, remote_path):\n",
    "    \"\"\"\n",
    "    Write DataFrame to target SFTP using custom data source\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to write\n",
    "        remote_path: Remote file path on SFTP\n",
    "    \"\"\"\n",
    "    # Convert to Pandas for writing (for batch processing)\n",
    "    pdf = df.toPandas()\n",
    "    \n",
    "    # Write to SFTP\n",
    "    writer = SFTPDataSource.create_writer(target_sftp_config)\n",
    "    with writer.session():\n",
    "        writer.write_dataframe(\n",
    "            pdf,\n",
    "            remote_path,\n",
    "            format=\"csv\",\n",
    "            header=True\n",
    "        )\n",
    "    \n",
    "    print(f\"Data written to SFTP: {remote_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dlt.table(\n",
    "    name=\"target_customer_summary\",\n",
    "    comment=\"Customer summary data for SFTP export\",\n",
    "    table_properties={\"quality\": \"gold\"}\n",
    ")\n",
    "def target_customer_summary():\n",
    "    \"\"\"Prepare customer summary for SFTP export\"\"\"\n",
    "    return dlt.read(\"gold_customer_summary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Export gold layer data to target SFTP\n# This would typically be triggered after the DLT pipeline completes\n\ndef export_to_sftp():\n    \"\"\"\n    Export processed data to target SFTP\n    Run this after DLT pipeline completes\n    \"\"\"\n    # Read gold layer data\n    customer_summary_df = spark.table(f\"{CATALOG_NAME}.gold.gold_customer_summary\")\n    customer_orders_df = spark.table(f\"{CATALOG_NAME}.gold.gold_customer_orders\")\n    \n    # Write to target SFTP\n    write_to_sftp(customer_summary_df, \"/customer_summary.csv\")\n    write_to_sftp(customer_orders_df, \"/customer_orders.csv\")\n    \n    print(\"Export to target SFTP completed successfully\")\n\n# Uncomment to run export after pipeline\n# export_to_sftp()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verification: Read from Target SFTP\n",
    "\n",
    "Verify that data was successfully written to target SFTP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def verify_target_sftp():\n    \"\"\"\n    Verify data in target SFTP\n    \"\"\"\n    writer = SFTPDataSource.create_writer(target_sftp_config)\n    \n    with writer.session():\n        files = writer.list_files(\".\")\n        print(\"Files in target SFTP:\")\n        for f in files:\n            print(f\"  - {f}\")\n    \n    # Read back using AutoLoader to verify\n    verification_df = (\n        spark.read\n        .format(\"csv\")\n        .option(\"header\", \"true\")\n        .option(\"inferSchema\", \"true\")\n        .load(f\"sftp://{TARGET_HOST}/customer_summary.csv\")\n    )\n    \n    print(\"\\nSample data from target SFTP:\")\n    verification_df.show(10)\n    \n    print(f\"\\nTotal records in target: {verification_df.count()}\")\n    \n    # Clean up temporary key file\n    if os.path.exists(tmp_key_file.name):\n        os.remove(tmp_key_file.name)\n        print(f\"\\n✓ Cleaned up temporary SSH key file\")\n\n# Uncomment to run verification\n# verify_target_sftp()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Pipeline Execution Summary\n\nTo run this complete pipeline:\n\n1. **Set Widget Values:**\n   - Set `catalog_name` widget to match your catalog (default: `sftp_demo`)\n   - Set `schema_name` widget to match your schema (default: `default`)\n\n2. **Create DLT Pipeline in Databricks:**\n   - Go to Delta Live Tables → Create Pipeline\n   - Set notebook path to this notebook\n   - Configure target catalog using the widget value\n   - Set storage location for checkpoints\n   - Enable AutoLoader for continuous ingestion\n\n3. **Pipeline Configuration:**\n   Configure the DLT pipeline to use your catalog and schema names.\n\n4. **Start Pipeline:**\n   - Click \"Start\" to run the pipeline\n   - Monitor data flow: Bronze → Silver → Gold\n   - Check data quality metrics\n\n5. **Export to Target SFTP:**\n   - After pipeline completes, run `export_to_sftp()`\n   - Verify with `verify_target_sftp()`\n\n### Data Flow:\n```\nSource SFTP (customers.csv, orders.csv)\n    ↓ AutoLoader\nBronze Layer (raw data)\n    ↓ Cleaning & Validation\nSilver Layer (validated data)\n    ↓ Enrichment & Aggregation\nGold Layer (business metrics)\n    ↓ Custom SFTP Data Source\nTarget SFTP (customer_summary.csv, customer_orders.csv)\n```\n\n### Success Criteria:\n- ✓ Bronze tables contain all source records\n- ✓ Silver tables have clean, validated data\n- ✓ Gold tables contain enriched business metrics\n- ✓ Data quality expectations passed\n- ✓ Target SFTP contains exported files\n- ✓ Data lineage visible in DLT UI"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}