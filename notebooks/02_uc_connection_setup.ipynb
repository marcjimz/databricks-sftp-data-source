{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 - Unity Catalog Connection Setup\n",
    "\n",
    "This notebook configures Unity Catalog connections for SFTP data sources:\n",
    "- Create Unity Catalog connections for source and target SFTP\n",
    "- Test connections using AutoLoader\n",
    "- Configure external locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Load Configuration"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# No imports needed - using built-in Databricks functionality"
  },
  {
   "cell_type": "markdown",
   "source": "## 1. Configure Catalog and Schema\n\nSet the catalog and schema names. These should match the values used in notebook 01.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Create widgets for catalog and schema configuration\ndbutils.widgets.text(\"catalog_name\", \"sftp_demo\", \"Catalog Name\")\ndbutils.widgets.text(\"schema_name\", \"default\", \"Schema Name\")\n\n# Get widget values\nCATALOG_NAME = dbutils.widgets.get(\"catalog_name\")\nSCHEMA_NAME = dbutils.widgets.get(\"schema_name\")\n\nprint(f\"Catalog: {CATALOG_NAME}\")\nprint(f\"Schema: {SCHEMA_NAME}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Load Configuration from Previous Setup"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load configuration\nconfig_df = spark.table(f\"{CATALOG_NAME}.config.connection_params\")\nconfig_dict = {row.key: row.value for row in config_df.collect()}\n\n# Get catalog and schema from config (verify they match)\ncatalog_name = config_dict.get(\"catalog_name\", CATALOG_NAME)\nschema_name = config_dict.get(\"schema_name\", SCHEMA_NAME)\n\nsource_host = config_dict[\"source_host\"]\nsource_username = config_dict[\"source_username\"]\ntarget_host = config_dict[\"target_host\"]\ntarget_username = config_dict[\"target_username\"]\nsecret_scope = config_dict[\"secret_scope\"]\nssh_key_secret = config_dict[\"ssh_key_secret\"]\n\nprint(\"Configuration loaded successfully\")\nprint(f\"Catalog: {catalog_name}\")\nprint(f\"Schema: {schema_name}\")\nprint(f\"Secret scope: {secret_scope}\")\nprint(f\"SSH key secret: {ssh_key_secret}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Unity Catalog Connection for Source SFTP\n",
    "\n",
    "**Note:** This requires Databricks workspace admin privileges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create source SFTP connection using SQL\nspark.sql(f\"\"\"\nCREATE CONNECTION IF NOT EXISTS {CATALOG_NAME}.source_sftp_connection\nTYPE sftp\nOPTIONS (\n  host '{source_host}',\n  port '22',\n  username '{source_username}',\n  privateKey SECRET ('{secret_scope}', '{ssh_key_secret}')\n)\n\"\"\")\n\nprint(f\"Source SFTP connection created: {CATALOG_NAME}.source_sftp_connection\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Unity Catalog Connection for Target SFTP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create target SFTP connection using SQL\nspark.sql(f\"\"\"\nCREATE CONNECTION IF NOT EXISTS {CATALOG_NAME}.target_sftp_connection\nTYPE sftp\nOPTIONS (\n  host '{target_host}',\n  port '22',\n  username '{target_username}',\n  privateKey SECRET ('{secret_scope}', '{ssh_key_secret}')\n)\n\"\"\")\n\nprint(f\"Target SFTP connection created: {CATALOG_NAME}.target_sftp_connection\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Verify Connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# List all connections\nconnections_df = spark.sql(f\"SHOW CONNECTIONS IN {CATALOG_NAME}\")\ndisplay(connections_df)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test Source Connection with AutoLoader\n",
    "\n",
    "Read data from source SFTP using AutoLoader to verify the connection works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test reading customers.csv from source SFTP\nsource_path = f\"sftp://{source_host}/customers.csv\"\n\ncustomers_df = (\n    spark.readStream\n    .format(\"cloudFiles\")\n    .option(\"cloudFiles.format\", \"csv\")\n    .option(\"cloudFiles.connectionName\", f\"{CATALOG_NAME}.source_sftp_connection\")\n    .option(\"header\", \"true\")\n    .option(\"inferSchema\", \"true\")\n    .load(source_path)\n)\n\n# Display schema\ncustomers_df.printSchema()\n\n# Write to temporary table for verification\n(\n    customers_df.writeStream\n    .format(\"memory\")\n    .queryName(\"test_customers\")\n    .outputMode(\"append\")\n    .start()\n)\n\nprint(\"Source SFTP connection verified successfully\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample data\n",
    "display(spark.sql(\"SELECT * FROM test_customers LIMIT 10\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create Catalog and Schema for Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create catalog structure for DLT pipeline\nspark.sql(f\"CREATE CATALOG IF NOT EXISTS {CATALOG_NAME}\")\nspark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG_NAME}.bronze\")\nspark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG_NAME}.silver\")\nspark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG_NAME}.gold\")\nspark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG_NAME}.{SCHEMA_NAME}\")\n\nprint(\"Catalog structure created:\")\nprint(f\"  - {CATALOG_NAME}.bronze (raw data from source SFTP)\")\nprint(f\"  - {CATALOG_NAME}.silver (cleaned and validated data)\")\nprint(f\"  - {CATALOG_NAME}.gold (aggregated business-level data)\")\nprint(f\"  - {CATALOG_NAME}.{SCHEMA_NAME} (default schema)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Create External Location for Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create checkpoint location in DBFS\ncheckpoint_location = f\"/dbfs/{CATALOG_NAME}/checkpoints\"\ndbutils.fs.mkdirs(checkpoint_location)\n\nprint(f\"Checkpoint location created: {checkpoint_location}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Grant Permissions (if needed)\n",
    "\n",
    "Grant necessary permissions to use the connections in DLT pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Grant USAGE on connections to all users (adjust as needed)\n# Uncomment if you need to grant permissions:\n\n# spark.sql(f\"\"\"\n# GRANT USAGE ON CONNECTION {CATALOG_NAME}.source_sftp_connection \n# TO `account users`\n# \"\"\")\n\n# spark.sql(f\"\"\"\n# GRANT USAGE ON CONNECTION {CATALOG_NAME}.target_sftp_connection \n# TO `account users`\n# \"\"\")\n\nprint(\"Connection permissions configured\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Test Complete Data Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Read orders.csv from source SFTP\norders_path = f\"sftp://{source_host}/orders.csv\"\n\norders_df = (\n    spark.readStream\n    .format(\"cloudFiles\")\n    .option(\"cloudFiles.format\", \"csv\")\n    .option(\"cloudFiles.connectionName\", f\"{CATALOG_NAME}.source_sftp_connection\")\n    .option(\"header\", \"true\")\n    .option(\"inferSchema\", \"true\")\n    .load(orders_path)\n)\n\n# Display schema\norders_df.printSchema()\n\n# Write to temporary table\n(\n    orders_df.writeStream\n    .format(\"memory\")\n    .queryName(\"test_orders\")\n    .outputMode(\"append\")\n    .start()\n)\n\nprint(\"Orders data loaded successfully\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample orders data\n",
    "display(spark.sql(\"SELECT * FROM test_orders LIMIT 10\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Unity Catalog connection setup completed:\n",
    "- ✓ Source SFTP connection created and tested\n",
    "- ✓ Target SFTP connection created\n",
    "- ✓ Catalog and schema structure created (bronze, silver, gold)\n",
    "- ✓ AutoLoader successfully reading from source SFTP\n",
    "- ✓ Checkpoint locations configured\n",
    "\n",
    "Next step: Run notebook `03_dlt_pipeline.ipynb` to create and execute the Delta Live Tables pipeline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}