{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1c98bef-c3a9-419f-99a3-4e8516ac4655",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 02 - Unity Catalog Connection Setup\n",
    "\n",
    "This notebook configures Unity Catalog connections for SFTP data sources and demonstrates both reading and writing with SFTP.\n",
    "\n",
    "**Note:** AutoLoader with SFTP is straightforward and built-in to Databricks. The main purpose of this repository is to demonstrate the **custom SFTPWriter** for writing data back to SFTP servers.\n",
    "\n",
    "This notebook will:\n",
    "- Create Unity Catalog SFTP connections (stores credentials securely)\n",
    "- Verify AutoLoader can read from SFTP using simple URI format\n",
    "- **Demonstrate custom SFTPWriter API for writing to SFTP**\n",
    "- Create catalog and schema structure for the DLT pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8124699b-d1c9-4eda-8474-3f0d0a2ee40d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Install dependencies from requirements.txt\n",
    "%pip install -r ../requirements.txt\n",
    "#%pip install -q -e ../\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8fb2bd02-fd6b-48a0-8500-293ef9c780a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Load Configuration from Previous Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "689e7c44-774a-4b83-9190-f3a3fef9f575",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create widgets for catalog and schema configuration\n",
    "dbutils.widgets.text(\"catalog_name\", \"sftp_demo\", \"Catalog Name\")\n",
    "dbutils.widgets.text(\"schema_name\", \"default\", \"Schema Name\")\n",
    "dbutils.widgets.text(\"source_connection_name\", \"source_sftp_connection\", \"Source Connection Name\")\n",
    "dbutils.widgets.text(\"target_connection_name\", \"target_sftp_connection\", \"Target Connection Name\")\n",
    "\n",
    "# Get widget values\n",
    "CATALOG_NAME = dbutils.widgets.get(\"catalog_name\")\n",
    "SCHEMA_NAME = dbutils.widgets.get(\"schema_name\")\n",
    "SOURCE_CONNECTION_NAME = dbutils.widgets.get(\"source_connection_name\")\n",
    "TARGET_CONNECTION_NAME = dbutils.widgets.get(\"target_connection_name\")\n",
    "\n",
    "print(f\"Catalog: {CATALOG_NAME}\")\n",
    "print(f\"Schema: {SCHEMA_NAME}\")\n",
    "print(f\"Source Connection: {SOURCE_CONNECTION_NAME}\")\n",
    "print(f\"Target Connection: {TARGET_CONNECTION_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60e439d2-9996-46a5-9dd5-70458dff8abc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config_df = spark.table(f\"{CATALOG_NAME}.config.connection_params\")\n",
    "config_dict = {row.key: row.value for row in config_df.collect()}\n",
    "\n",
    "# Get configuration values\n",
    "catalog_name = config_dict.get(\"catalog_name\", CATALOG_NAME)\n",
    "schema_name = config_dict.get(\"schema_name\", SCHEMA_NAME)\n",
    "source_connection_name = config_dict.get(\"source_connection_name\", SOURCE_CONNECTION_NAME)\n",
    "target_connection_name = config_dict.get(\"target_connection_name\", TARGET_CONNECTION_NAME)\n",
    "\n",
    "source_host = config_dict[\"source_host\"]\n",
    "source_username = config_dict[\"source_username\"]\n",
    "target_host = config_dict[\"target_host\"]\n",
    "target_username = config_dict[\"target_username\"]\n",
    "secret_scope = config_dict[\"secret_scope\"]\n",
    "ssh_key_secret = config_dict[\"ssh_key_secret\"]\n",
    "ssh_key_fingerprint = config_dict[\"ssh_key_fingerprint\"]\n",
    "\n",
    "print(\"Configuration loaded successfully\")\n",
    "print(f\"Catalog: {catalog_name}\")\n",
    "print(f\"Schema: {schema_name}\")\n",
    "print(f\"Source Connection: {source_connection_name}\")\n",
    "print(f\"Target Connection: {target_connection_name}\")\n",
    "print(f\"Secret scope: {secret_scope}\")\n",
    "print(f\"SSH key secret: {ssh_key_secret}\")\n",
    "print(f\"SSH key fingerprint: {ssh_key_fingerprint}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dbe6960b-3ff0-481a-b062-19c233ffaf1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. Create Unity Catalog Connection for Source SFTP\n",
    "\n",
    "**Note:** This requires Databricks workspace admin privileges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24c9fcd2-8450-49e7-be8c-1fd60bc7b0eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set catalog context first\n",
    "spark.sql(f\"USE CATALOG {CATALOG_NAME}\")\n",
    "\n",
    "# Debug: Print the values being used\n",
    "print(\"Creating source SFTP connection with:\")\n",
    "print(f\"  host: {source_host}\")\n",
    "print(f\"  port: 22\")\n",
    "print(f\"  user: SECRET('{secret_scope}', 'source-username')\")\n",
    "print(f\"  pem_private_key: SECRET('{secret_scope}', '{ssh_key_secret}')\")\n",
    "print(f\"  key_fingerprint: {ssh_key_fingerprint}\")\n",
    "\n",
    "# Build the SQL statement\n",
    "create_source_sql = f\"\"\"\n",
    "CREATE CONNECTION IF NOT EXISTS {SOURCE_CONNECTION_NAME}\n",
    "TYPE sftp\n",
    "OPTIONS (\n",
    "  host '{source_host}',\n",
    "  port '22',\n",
    "  user SECRET ('{secret_scope}', 'source-username'),\n",
    "  pem_private_key SECRET ('{secret_scope}', '{ssh_key_secret}'),\n",
    "  key_fingerprint '{ssh_key_fingerprint}'\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "# Create source SFTP connection\n",
    "spark.sql(create_source_sql)\n",
    "\n",
    "print(f\"\\n✓ Source SFTP connection created: {SOURCE_CONNECTION_NAME} (in catalog {CATALOG_NAME})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e1b67ba7-9f58-4a41-8fcd-d416b35ab112",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. Create Unity Catalog Connection for Target SFTP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ccdf1f2-96f5-409c-8ea2-e22da18eb70c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Debug: Print the values being used\n",
    "print(\"Creating target SFTP connection with:\")\n",
    "print(f\"  host: {target_host}\")\n",
    "print(f\"  port: 22\")\n",
    "print(f\"  user: SECRET('{secret_scope}', 'target-username')\")\n",
    "print(f\"  pem_private_key: SECRET('{secret_scope}', '{ssh_key_secret}')\")\n",
    "print(f\"  key_fingerprint: {ssh_key_fingerprint}\")\n",
    "\n",
    "# Build the SQL statement\n",
    "create_target_sql = f\"\"\"\n",
    "CREATE CONNECTION IF NOT EXISTS {TARGET_CONNECTION_NAME}\n",
    "TYPE sftp\n",
    "OPTIONS (\n",
    "  host '{target_host}',\n",
    "  port '22',\n",
    "  user SECRET ('{secret_scope}', 'target-username'),\n",
    "  pem_private_key SECRET ('{secret_scope}', '{ssh_key_secret}'),\n",
    "  key_fingerprint '{ssh_key_fingerprint}'\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "# Create target SFTP connection\n",
    "spark.sql(create_target_sql)\n",
    "\n",
    "print(f\"\\n✓ Target SFTP connection created: {TARGET_CONNECTION_NAME} (in catalog {CATALOG_NAME})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e3c55f3b-a0de-4679-a4fa-ab68e5166aea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. Verify AutoLoader with SFTP\n",
    "\n",
    "AutoLoader automatically finds the Unity Catalog connection based on the host in the SFTP URI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6fb13c6-6381-404e-b02f-e93a841d33e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Test reading customers.csv from source SFTP using AutoLoader\n",
    "# AutoLoader automatically finds the connection based on the host in the URI\n",
    "source_sftp_uri = f\"sftp://{source_username}@{source_host}:22/customers.csv\"\n",
    "\n",
    "customers_df = (\n",
    "    spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"csv\")\n",
    "    .option(\"cloudFiles.schemaLocation\", f\"/tmp/{CATALOG_NAME}/schema/customers\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .load(source_sftp_uri)\n",
    ")\n",
    "\n",
    "# Display schema\n",
    "print(\"Schema:\")\n",
    "customers_df.printSchema()\n",
    "\n",
    "# Write to temporary table for verification (use availableNow for serverless)\n",
    "query = (\n",
    "    customers_df.writeStream\n",
    "    .format(\"memory\")\n",
    "    .queryName(\"test_customers\")\n",
    "    .outputMode(\"append\")\n",
    "    .trigger(availableNow=True)\n",
    "    .start()\n",
    ")\n",
    "\n",
    "# Wait for the micro-batch to complete\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ef58293-0e70-4cba-a216-e371199f11b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Display sample data\n",
    "print(\"\\nSample data:\")\n",
    "display(spark.sql(\"SELECT * FROM test_customers LIMIT 10\"))\n",
    "\n",
    "print(f\"\\n✓ Source SFTP AutoLoader verified successfully\")\n",
    "print(f\"  URI: {source_sftp_uri}\")\n",
    "print(f\"  Connection matched automatically by host: {source_host}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e8a1a07-b2f2-467e-a6ac-cec00ce1f39b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5. Demonstrate Custom SFTPWriter API\n",
    "\n",
    "The main focus of this repository is the **custom SFTPWriter** for writing data to SFTP using **Paramiko**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f893bc7-d05d-4e7f-ad31-629628850bd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "# Add src folder to Python path\n",
    "notebook_path = os.path.dirname(dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get())\n",
    "repo_root = os.path.dirname(notebook_path)\n",
    "src_path = os.path.join(repo_root, 'src')\n",
    "\n",
    "if src_path not in sys.path:\n",
    "    sys.path.insert(0, src_path)\n",
    "\n",
    "# Import custom SFTP Data Source\n",
    "from src.ingest import SFTPDataSource\n",
    "\n",
    "# Register the SFTP data source with Spark\n",
    "spark.dataSource.register(SFTPDataSource)\n",
    "\n",
    "print(f\"✓ Custom SFTP package imported from: {src_path}\")\n",
    "print(f\"✓ SFTP data source registered with Spark\")\n",
    "print(f\"  Usage: df.write.format('sftp').option(...).save()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f2577795-ce53-4e28-b91b-40bf8cc80f08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create demo DataFrame to write to target SFTP\n",
    "from datetime import datetime\n",
    "\n",
    "demo_data = [\n",
    "    (1, \"Demo Customer 1\", \"demo1@example.com\", \"USA\", datetime.now().strftime(\"%Y-%m-%d\")),\n",
    "    (2, \"Demo Customer 2\", \"demo2@example.com\", \"UK\", datetime.now().strftime(\"%Y-%m-%d\")),\n",
    "    (3, \"Demo Customer 3\", \"demo3@example.com\", \"Canada\", datetime.now().strftime(\"%Y-%m-%d\"))\n",
    "]\n",
    "\n",
    "demo_df = spark.createDataFrame(demo_data, [\"customer_id\", \"name\", \"email\", \"country\", \"signup_date\"])\n",
    "\n",
    "print(\"Demo DataFrame created:\")\n",
    "demo_df.show()\n",
    "\n",
    "# Convert to Pandas for writing with SFTPWriter\n",
    "demo_pdf = demo_df.toPandas()\n",
    "\n",
    "print(f\"\\n✓ Created demo DataFrame with {len(demo_pdf)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5837d4bf-33a8-49ba-b169-11f61595efbc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write demo data to target SFTP using Databricks Python Data Source API\n",
    "# This demonstrates the proper Spark-based approach with Paramiko\n",
    "\n",
    "# Get SSH private key from secrets and write to temporary file\n",
    "ssh_key_content = dbutils.secrets.get(scope=secret_scope, key=ssh_key_secret)\n",
    "tmp_key_file = tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='_sftp_key')\n",
    "tmp_key_file.write(ssh_key_content)\n",
    "tmp_key_file.close()\n",
    "os.chmod(tmp_key_file.name, 0o600)\n",
    "\n",
    "remote_path = \"/demo_customers.csv\"\n",
    "\n",
    "print(f\"Writing demo DataFrame to SFTP using Spark DataSource API\")\n",
    "print(f\"Target: {target_username}@{target_host}{remote_path}\")\n",
    "print(f\"Technology: Paramiko SSHv2 library (version 3.4.0)\\n\")\n",
    "\n",
    "# Write using Spark DataSource API - THIS IS THE PROPER WAY\n",
    "demo_df.write \\\\\n",
    "    .format(\"sftp\") \\\\\n",
    "    .option(\"host\", target_host) \\\\\n",
    "    .option(\"username\", target_username) \\\\\n",
    "    .option(\"private_key_path\", tmp_key_file.name) \\\\\n",
    "    .option(\"port\", \"22\") \\\\\n",
    "    .option(\"path\", remote_path) \\\\\n",
    "    .option(\"format\", \"csv\") \\\\\n",
    "    .option(\"header\", \"true\") \\\\\n",
    "    .mode(\"overwrite\") \\\\\n",
    "    .save()\n",
    "\n",
    "# Clean up temporary SSH key file\n",
    "if os.path.exists(tmp_key_file.name):\n",
    "    os.remove(tmp_key_file.name)\n",
    "    print(f\"\\n✓ Cleaned up temporary SSH key file\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Custom SFTP Data Source Demo Complete\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Technology: Paramiko SSHv2 library\")\n",
    "print(f\"API: Databricks Python Data Source API\")\n",
    "print(f\"Pattern: spark.dataSource.register() + df.write.format('sftp')\")\n",
    "print(f\"Written: {demo_df.count()} rows to {remote_path}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "011e3676-2957-41ef-bc21-cf3e25c779d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 6. Create Catalog Structure for DLT Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb769e58-f1e4-4501-a487-e3245af322ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create catalog structure for DLT pipeline\n",
    "spark.sql(f\"CREATE CATALOG IF NOT EXISTS {CATALOG_NAME}\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG_NAME}.bronze\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG_NAME}.silver\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG_NAME}.gold\")\n",
    "\n",
    "print(\"Catalog structure created:\")\n",
    "print(f\"  - {CATALOG_NAME}.bronze (raw data from source SFTP)\")\n",
    "print(f\"  - {CATALOG_NAME}.silver (cleaned and validated data)\")\n",
    "print(f\"  - {CATALOG_NAME}.gold (aggregated business-level data)\")\n",
    "print(f\"  - {CATALOG_NAME}.{SCHEMA_NAME} (default schema)\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "02_uc_connection_setup",
   "widgets": {
    "catalog_name": {
     "currentValue": "sftp_demo",
     "nuid": "db34bd33-38e4-4f9d-8d6b-7fb16946d07c",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "sftp_demo",
      "label": "Catalog Name",
      "name": "catalog_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "sftp_demo",
      "label": "Catalog Name",
      "name": "catalog_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "schema_name": {
     "currentValue": "default",
     "nuid": "cbbc9a4e-3cbf-46a3-8c21-ce0f12097d27",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "default",
      "label": "Schema Name",
      "name": "schema_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "default",
      "label": "Schema Name",
      "name": "schema_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "source_connection_name": {
     "currentValue": "source_sftp_connection",
     "nuid": "4ecdf67a-70b9-4910-b4e6-a822a61ec3e5",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "source_sftp_connection",
      "label": "Source Connection Name",
      "name": "source_connection_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "source_sftp_connection",
      "label": "Source Connection Name",
      "name": "source_connection_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "target_connection_name": {
     "currentValue": "target_sftp_connection",
     "nuid": "e253f410-6f6d-4144-8ecc-a3074cb077aa",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "target_sftp_connection",
      "label": "Target Connection Name",
      "name": "target_connection_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "target_sftp_connection",
      "label": "Target Connection Name",
      "name": "target_connection_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
