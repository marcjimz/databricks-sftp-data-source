{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1c98bef-c3a9-419f-99a3-4e8516ac4655",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": "# 02 - Unity Catalog Connection Setup\n\nThis notebook configures Unity Catalog connections for SFTP data sources and demonstrates both reading and writing with SFTP.\n\n**Note:** AutoLoader with SFTP is straightforward and built-in to Databricks. The main purpose of this repository is to demonstrate the **custom SFTPWriter** for writing data back to SFTP servers.\n\nThis notebook will:\n- Create Unity Catalog SFTP connections (stores credentials securely)\n- Verify AutoLoader can read from SFTP using simple URI format\n- **Demonstrate custom SFTPWriter API for writing to SFTP**\n- Create catalog and schema structure for the DLT pipeline"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8fb2bd02-fd6b-48a0-8500-293ef9c780a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Load Configuration from Previous Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "689e7c44-774a-4b83-9190-f3a3fef9f575",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create widgets for catalog and schema configuration\n",
    "dbutils.widgets.text(\"catalog_name\", \"sftp_demo\", \"Catalog Name\")\n",
    "dbutils.widgets.text(\"schema_name\", \"default\", \"Schema Name\")\n",
    "dbutils.widgets.text(\"source_connection_name\", \"source_sftp_connection\", \"Source Connection Name\")\n",
    "dbutils.widgets.text(\"target_connection_name\", \"target_sftp_connection\", \"Target Connection Name\")\n",
    "\n",
    "# Get widget values\n",
    "CATALOG_NAME = dbutils.widgets.get(\"catalog_name\")\n",
    "SCHEMA_NAME = dbutils.widgets.get(\"schema_name\")\n",
    "SOURCE_CONNECTION_NAME = dbutils.widgets.get(\"source_connection_name\")\n",
    "TARGET_CONNECTION_NAME = dbutils.widgets.get(\"target_connection_name\")\n",
    "\n",
    "print(f\"Catalog: {CATALOG_NAME}\")\n",
    "print(f\"Schema: {SCHEMA_NAME}\")\n",
    "print(f\"Source Connection: {SOURCE_CONNECTION_NAME}\")\n",
    "print(f\"Target Connection: {TARGET_CONNECTION_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60e439d2-9996-46a5-9dd5-70458dff8abc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config_df = spark.table(f\"{CATALOG_NAME}.config.connection_params\")\n",
    "config_dict = {row.key: row.value for row in config_df.collect()}\n",
    "\n",
    "# Get configuration values\n",
    "catalog_name = config_dict.get(\"catalog_name\", CATALOG_NAME)\n",
    "schema_name = config_dict.get(\"schema_name\", SCHEMA_NAME)\n",
    "source_connection_name = config_dict.get(\"source_connection_name\", SOURCE_CONNECTION_NAME)\n",
    "target_connection_name = config_dict.get(\"target_connection_name\", TARGET_CONNECTION_NAME)\n",
    "\n",
    "source_host = config_dict[\"source_host\"]\n",
    "source_username = config_dict[\"source_username\"]\n",
    "target_host = config_dict[\"target_host\"]\n",
    "target_username = config_dict[\"target_username\"]\n",
    "secret_scope = config_dict[\"secret_scope\"]\n",
    "ssh_key_secret = config_dict[\"ssh_key_secret\"]\n",
    "ssh_key_fingerprint = config_dict[\"ssh_key_fingerprint\"]\n",
    "\n",
    "print(\"Configuration loaded successfully\")\n",
    "print(f\"Catalog: {catalog_name}\")\n",
    "print(f\"Schema: {schema_name}\")\n",
    "print(f\"Source Connection: {source_connection_name}\")\n",
    "print(f\"Target Connection: {target_connection_name}\")\n",
    "print(f\"Secret scope: {secret_scope}\")\n",
    "print(f\"SSH key secret: {ssh_key_secret}\")\n",
    "print(f\"SSH key fingerprint: {ssh_key_fingerprint}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dbe6960b-3ff0-481a-b062-19c233ffaf1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. Create Unity Catalog Connection for Source SFTP\n",
    "\n",
    "**Note:** This requires Databricks workspace admin privileges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24c9fcd2-8450-49e7-be8c-1fd60bc7b0eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set catalog context first\n",
    "spark.sql(f\"USE CATALOG {CATALOG_NAME}\")\n",
    "\n",
    "# Debug: Print the values being used\n",
    "print(\"Creating source SFTP connection with:\")\n",
    "print(f\"  host: {source_host}\")\n",
    "print(f\"  port: 22\")\n",
    "print(f\"  user: SECRET('{secret_scope}', 'source-username')\")\n",
    "print(f\"  pem_private_key: SECRET('{secret_scope}', '{ssh_key_secret}')\")\n",
    "print(f\"  key_fingerprint: {ssh_key_fingerprint}\")\n",
    "\n",
    "# Build the SQL statement\n",
    "create_source_sql = f\"\"\"\n",
    "CREATE CONNECTION IF NOT EXISTS {SOURCE_CONNECTION_NAME}\n",
    "TYPE sftp\n",
    "OPTIONS (\n",
    "  host '{source_host}',\n",
    "  port '22',\n",
    "  user SECRET ('{secret_scope}', 'source-username'),\n",
    "  pem_private_key SECRET ('{secret_scope}', '{ssh_key_secret}'),\n",
    "  key_fingerprint '{ssh_key_fingerprint}'\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "# Create source SFTP connection\n",
    "spark.sql(create_source_sql)\n",
    "\n",
    "print(f\"\\n\u2713 Source SFTP connection created: {SOURCE_CONNECTION_NAME} (in catalog {CATALOG_NAME})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e1b67ba7-9f58-4a41-8fcd-d416b35ab112",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. Create Unity Catalog Connection for Target SFTP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ccdf1f2-96f5-409c-8ea2-e22da18eb70c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Debug: Print the values being used\n",
    "print(\"Creating target SFTP connection with:\")\n",
    "print(f\"  host: {target_host}\")\n",
    "print(f\"  port: 22\")\n",
    "print(f\"  user: SECRET('{secret_scope}', 'target-username')\")\n",
    "print(f\"  pem_private_key: SECRET('{secret_scope}', '{ssh_key_secret}')\")\n",
    "print(f\"  key_fingerprint: {ssh_key_fingerprint}\")\n",
    "\n",
    "# Build the SQL statement\n",
    "create_target_sql = f\"\"\"\n",
    "CREATE CONNECTION IF NOT EXISTS {TARGET_CONNECTION_NAME}\n",
    "TYPE sftp\n",
    "OPTIONS (\n",
    "  host '{target_host}',\n",
    "  port '22',\n",
    "  user SECRET ('{secret_scope}', 'target-username'),\n",
    "  pem_private_key SECRET ('{secret_scope}', '{ssh_key_secret}'),\n",
    "  key_fingerprint '{ssh_key_fingerprint}'\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "# Create target SFTP connection\n",
    "spark.sql(create_target_sql)\n",
    "\n",
    "print(f\"\\n\u2713 Target SFTP connection created: {TARGET_CONNECTION_NAME} (in catalog {CATALOG_NAME})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e3c55f3b-a0de-4679-a4fa-ab68e5166aea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. Verify AutoLoader with SFTP\n",
    "\n",
    "AutoLoader automatically finds the Unity Catalog connection based on the host in the SFTP URI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6fb13c6-6381-404e-b02f-e93a841d33e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": "# Test reading customers.csv from source SFTP using AutoLoader\n# AutoLoader automatically finds the connection based on the host in the URI\nsource_sftp_uri = f\"sftp://{source_username}@{source_host}:22/customers.csv\"\n\ncustomers_df = (\n    spark.readStream\n    .format(\"cloudFiles\")\n    .option(\"cloudFiles.format\", \"csv\")\n    .option(\"cloudFiles.schemaLocation\", f\"/tmp/{CATALOG_NAME}/schema/customers\")\n    .option(\"header\", \"true\")\n    .load(source_sftp_uri)\n)\n\n# Display schema\nprint(\"Schema:\")\ncustomers_df.printSchema()\n\n# Write to temporary table for verification (use availableNow for serverless)\nquery = (\n    customers_df.writeStream\n    .format(\"memory\")\n    .queryName(\"test_customers\")\n    .outputMode(\"append\")\n    .trigger(availableNow=True)\n    .start()\n)\n\n# Wait for the micro-batch to complete\nquery.awaitTermination()\n\n# Display sample data\nprint(\"\\nSample data:\")\ndisplay(spark.sql(\"SELECT * FROM test_customers LIMIT 10\"))\n\nprint(f\"\\n\u2713 Source SFTP AutoLoader verified successfully\")\nprint(f\"  URI: {source_sftp_uri}\")\nprint(f\"  Connection matched automatically by host: {source_host}\")"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Demonstrate Custom SFTPWriter API\n",
    "\n",
    "The main focus of this repository is the **custom SFTPWriter** for writing data to SFTP using **Paramiko**."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import sys\nimport os\nimport tempfile\n\n# Add src folder to Python path\nnotebook_path = os.path.dirname(dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get())\nrepo_root = os.path.dirname(notebook_path)\nsrc_path = os.path.join(repo_root, 'src')\n\nif src_path not in sys.path:\n    sys.path.insert(0, src_path)\n\n# Import custom SFTP package\nfrom ingest import SFTPWriter, SFTPDataSource\n\n# Get SSH private key from secrets and write to temporary file\nssh_key_content = dbutils.secrets.get(scope=secret_scope, key=ssh_key_secret)\ntmp_key_file = tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='_sftp_key')\ntmp_key_file.write(ssh_key_content)\ntmp_key_file.close()\nos.chmod(tmp_key_file.name, 0o600)\n\n# Configure target SFTP for writing\ntarget_sftp_config = {\n    \"host\": target_host,\n    \"username\": target_username,\n    \"private_key_path\": tmp_key_file.name,\n    \"port\": 22\n}\n\nprint(f\"\u2713 Custom SFTP package imported from: {src_path}\")\nprint(f\"\u2713 Target SFTP configured: {target_username}@{target_host}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Create demo DataFrame to write to target SFTP\nfrom datetime import datetime\n\ndemo_data = [\n    (1, \"Demo Customer 1\", \"demo1@example.com\", \"USA\", datetime.now().strftime(\"%Y-%m-%d\")),\n    (2, \"Demo Customer 2\", \"demo2@example.com\", \"UK\", datetime.now().strftime(\"%Y-%m-%d\")),\n    (3, \"Demo Customer 3\", \"demo3@example.com\", \"Canada\", datetime.now().strftime(\"%Y-%m-%d\"))\n]\n\ndemo_df = spark.createDataFrame(demo_data, [\"customer_id\", \"name\", \"email\", \"country\", \"signup_date\"])\n\nprint(\"Demo DataFrame created:\")\ndemo_df.show()\n\n# Convert to Pandas for writing with SFTPWriter\ndemo_pdf = demo_df.toPandas()\n\nprint(f\"\\n\u2713 Created demo DataFrame with {len(demo_pdf)} rows\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write demo data to target SFTP using custom SFTPWriter (Paramiko)\n",
    "remote_path = \"/demo_customers.csv\"\n",
    "\n",
    "print(f\"Writing demo data to target SFTP: {target_username}@{target_host}{remote_path}\")\n",
    "print(f\"Using Paramiko (version 3.4.0) for SFTP connection\\n\")\n",
    "\n",
    "# Use SFTPDataSource factory to create writer\n",
    "writer = SFTPDataSource.create_writer(target_sftp_config)\n",
    "\n",
    "# Write DataFrame using context manager pattern\n",
    "with writer.session():\n",
    "    # Write the Pandas DataFrame to SFTP as CSV\n",
    "    writer.write_dataframe(\n",
    "        demo_pdf,\n",
    "        remote_path,\n",
    "        format=\"csv\",\n",
    "        header=True\n",
    "    )\n",
    "    \n",
    "    print(f\"\u2713 Data written successfully to {remote_path}\")\n",
    "    \n",
    "    # List files on target SFTP to verify\n",
    "    print(\"\\nFiles on target SFTP:\")\n",
    "    files = writer.list_files(\".\")\n",
    "    for f in files:\n",
    "        print(f\"  - {f}\")\n",
    "\n",
    "# Clean up temporary SSH key file\n",
    "if os.path.exists(tmp_key_file.name):\n",
    "    os.remove(tmp_key_file.name)\n",
    "    print(f\"\\n\u2713 Cleaned up temporary SSH key file\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Custom SFTPWriter Demo Complete\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Technology: Paramiko SSHv2 library\")\n",
    "print(f\"Pattern: Factory + Context Manager\")\n",
    "print(f\"Written: {len(demo_pdf)} rows to {remote_path}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "011e3676-2957-41ef-bc21-cf3e25c779d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 6. Create Catalog Structure for DLT Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb769e58-f1e4-4501-a487-e3245af322ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create catalog structure for DLT pipeline\n",
    "spark.sql(f\"CREATE CATALOG IF NOT EXISTS {CATALOG_NAME}\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG_NAME}.bronze\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG_NAME}.silver\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG_NAME}.gold\")\n",
    "\n",
    "print(\"Catalog structure created:\")\n",
    "print(f\"  - {CATALOG_NAME}.bronze (raw data from source SFTP)\")\n",
    "print(f\"  - {CATALOG_NAME}.silver (cleaned and validated data)\")\n",
    "print(f\"  - {CATALOG_NAME}.gold (aggregated business-level data)\")\n",
    "print(f\"  - {CATALOG_NAME}.{SCHEMA_NAME} (default schema)\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "02_uc_connection_setup",
   "widgets": {
    "catalog_name": {
     "currentValue": "sftp_demo",
     "nuid": "db34bd33-38e4-4f9d-8d6b-7fb16946d07c",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "sftp_demo",
      "label": "Catalog Name",
      "name": "catalog_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "sftp_demo",
      "label": "Catalog Name",
      "name": "catalog_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "schema_name": {
     "currentValue": "default",
     "nuid": "cbbc9a4e-3cbf-46a3-8c21-ce0f12097d27",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "default",
      "label": "Schema Name",
      "name": "schema_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "default",
      "label": "Schema Name",
      "name": "schema_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "source_connection_name": {
     "currentValue": "source_sftp_connection",
     "nuid": "4ecdf67a-70b9-4910-b4e6-a822a61ec3e5",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "source_sftp_connection",
      "label": "Source Connection Name",
      "name": "source_connection_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "source_sftp_connection",
      "label": "Source Connection Name",
      "name": "source_connection_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "target_connection_name": {
     "currentValue": "target_sftp_connection",
     "nuid": "e253f410-6f6d-4144-8ecc-a3074cb077aa",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "target_sftp_connection",
      "label": "Target Connection Name",
      "name": "target_connection_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "target_sftp_connection",
      "label": "Target Connection Name",
      "name": "target_connection_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}