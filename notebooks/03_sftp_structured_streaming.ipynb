{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e4fd198-f238-4f01-87a0-bd4b13407685",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 03 - SFTP Structured Streaming Demo\n",
    "\n",
    "This notebook demonstrates reading and writing data with SFTP using:\n",
    "- **AutoLoader** for reading from SFTP (built-in Databricks feature)\n",
    "- **Custom SFTP Data Source** for writing to SFTP (Paramiko + Data Source API)\n",
    "\n",
    "**Prerequisites:**\n",
    "- Run notebook `01_infrastructure_setup.ipynb` first\n",
    "- Run notebook `02_uc_connection_setup.ipynb` second\n",
    "\n",
    "This notebook will:\n",
    "1. Read data from source SFTP using AutoLoader\n",
    "2. Write data to target SFTP using custom data source\n",
    "3. List files in target directory to verify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24796dfe-8199-404d-877a-7b7583f33a22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Install Package and Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "835fa48f-6eba-4d73-92bc-66454fde1d16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Install package from source (editable mode)\n",
    "%pip install -q -e ..\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe6d7308-1d0d-4d7f-9d9f-2cbbfa145a2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create widgets for catalog and schema configuration\n",
    "dbutils.widgets.text(\"catalog_name\", \"sftp_demo\", \"Catalog Name\")\n",
    "dbutils.widgets.text(\"schema_name\", \"default\", \"Schema Name\")\n",
    "\n",
    "# Get widget values\n",
    "CATALOG_NAME = dbutils.widgets.get(\"catalog_name\")\n",
    "SCHEMA_NAME = dbutils.widgets.get(\"schema_name\")\n",
    "\n",
    "print(f\"Catalog: {CATALOG_NAME}\")\n",
    "print(f\"Schema: {SCHEMA_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f790f7b-3b34-4d95-a628-a68533928139",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load configuration from catalog\n",
    "config_df = spark.table(f\"{CATALOG_NAME}.config.connection_params\")\n",
    "config_dict = {row.key: row.value for row in config_df.collect()}\n",
    "\n",
    "# Get configuration values\n",
    "source_host = config_dict[\"source_host\"]\n",
    "source_username = config_dict[\"source_username\"]\n",
    "target_host = config_dict[\"target_host\"]\n",
    "target_username = config_dict[\"target_username\"]\n",
    "secret_scope = config_dict[\"secret_scope\"]\n",
    "ssh_key_secret = config_dict[\"ssh_key_secret\"]\n",
    "\n",
    "print(\"✓ Configuration loaded successfully\")\n",
    "print(f\"  Source: {source_username}@{source_host}\")\n",
    "print(f\"  Target: {target_username}@{target_host}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffdd455b-85d0-41e5-9a45-9f018e8fdd01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. Read from SFTP using AutoLoader\n",
    "\n",
    "AutoLoader automatically finds the Unity Catalog connection based on the host in the SFTP URI.\n",
    "\n",
    "This uses:\n",
    "- **Checkpoint location**: Managed volume (`/Volumes/{catalog}/{schema}/_checkpoints`)\n",
    "- **Schema inference**: AutoLoader automatically infers and evolves the schema\n",
    "- **Serverless trigger**: `availableNow=True` for single micro-batch processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ed9bca3-835a-4caf-800f-3e5ff1e9b244",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Test reading customers.csv from source SFTP using AutoLoader\n",
    "source_sftp_uri = f\"sftp://{source_username}@{source_host}:22/customers.csv\"\n",
    "\n",
    "# Use managed volume for checkpoint location\n",
    "checkpoint_location = f\"/Volumes/{CATALOG_NAME}/{SCHEMA_NAME}/_checkpoints/customers\"\n",
    "schema_location = f\"/Volumes/{CATALOG_NAME}/{SCHEMA_NAME}/_checkpoints/customers_schema\"\n",
    "\n",
    "print(f\"AutoLoader configuration:\")\n",
    "print(f\"  Source URI: {source_sftp_uri}\")\n",
    "print(f\"  Checkpoint: {checkpoint_location}\")\n",
    "print(f\"  Schema: {schema_location}\\n\")\n",
    "\n",
    "customers_df = (\n",
    "    spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"csv\")\n",
    "    .option(\"cloudFiles.schemaLocation\", schema_location)\n",
    "    .option(\"header\", \"true\")\n",
    "    .load(source_sftp_uri)\n",
    ")\n",
    "\n",
    "# Display schema\n",
    "print(\"Schema:\")\n",
    "customers_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "865e0afd-6c7a-4261-a555-56959db5316b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write streaming data to table using checkpoint location in managed volume\n",
    "table_name = f\"{CATALOG_NAME}.{SCHEMA_NAME}.test_customers\"\n",
    "\n",
    "print(f\"Writing to table: {table_name}\")\n",
    "print(f\"Checkpoint location: {checkpoint_location}\\n\")\n",
    "\n",
    "query = (\n",
    "    customers_df.writeStream\n",
    "    .trigger(availableNow=True)\n",
    "    .option(\"checkpointLocation\", checkpoint_location)\n",
    "    .toTable(table_name)\n",
    ")\n",
    "\n",
    "# Wait for the micro-batch to complete\n",
    "query.awaitTermination()\n",
    "\n",
    "print(\"✅ All data processed!\")\n",
    "print(f\"\\nData read from SFTP:\")\n",
    "spark.sql(f\"SELECT * FROM {table_name}\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c8fb231-85da-4112-bf9d-28d40aabafc9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. Write to SFTP using Custom Data Source\n",
    "\n",
    "The custom SFTP Data Source uses:\n",
    "- **Databricks Python Data Source API** - Native Spark integration\n",
    "- **Paramiko 3.4.0** - Secure SSH/SFTP protocol implementation\n",
    "- **Distributed writes** - Each Spark executor creates its own temp key file\n",
    "\n",
    "This enables writing using the standard Spark API: `df.write.format(\"sftp\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33503e92-ed69-4bfa-bc0d-80925ba56a79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import os\n",
    "\n",
    "# Import custom SFTP Data Source\n",
    "from CustomDataSource import SFTPDataSource\n",
    "\n",
    "# Register the SFTP data source with Spark\n",
    "try:\n",
    "  spark.dataSource.register(SFTPDataSource)\n",
    "except AttributeError as e:\n",
    "  print(\"Please ensure you run Serverless Environment version 4 or greater!\")\n",
    "  raise e\n",
    "\n",
    "print(\"✓ SFTP data source registered with Spark\")\n",
    "print(f\"  Format name: 'sftp'\")\n",
    "print(f\"  Usage: df.write.format('sftp').option(...).save()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d85e078-e8d7-4251-b8fb-cf7509dbb4e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create demo DataFrame to write to target SFTP\n",
    "from datetime import datetime\n",
    "\n",
    "demo_data = [\n",
    "    (1, \"Demo Customer 1\", \"demo1@example.com\", \"USA\", datetime.now().strftime(\"%Y-%m-%d\")),\n",
    "    (2, \"Demo Customer 2\", \"demo2@example.com\", \"UK\", datetime.now().strftime(\"%Y-%m-%d\")),\n",
    "    (3, \"Demo Customer 3\", \"demo3@example.com\", \"Canada\", datetime.now().strftime(\"%Y-%m-%d\"))\n",
    "]\n",
    "\n",
    "demo_df = spark.createDataFrame(demo_data, [\"customer_id\", \"name\", \"email\", \"country\", \"signup_date\"])\n",
    "\n",
    "print(\"Demo DataFrame created:\")\n",
    "demo_df.show()\n",
    "\n",
    "print(f\"\\n✓ Created demo DataFrame with {demo_df.count()} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efc8c133-1148-4c7b-b859-fd45ad61adf1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write demo data to target SFTP using Databricks Python Data Source API\n",
    "# The SSH key content is passed as an option - each Spark executor creates its own temp file\n",
    "\n",
    "# Get SSH private key content from secrets\n",
    "ssh_key_content = dbutils.secrets.get(scope=secret_scope, key=ssh_key_secret)\n",
    "\n",
    "remote_path = \"/demo_customers.csv\"\n",
    "\n",
    "print(f\"Writing demo DataFrame to SFTP using Spark DataSource API\")\n",
    "print(f\"Target: {target_username}@{target_host}{remote_path}\")\n",
    "print(f\"Technology: Paramiko SSHv2 library (version 3.4.0)\")\n",
    "\n",
    "# Write using Spark DataSource API - passes key content, not path\n",
    "# Each executor will create its own temporary key file from the content\n",
    "demo_df.write \\\n",
    "    .format(\"sftp\") \\\n",
    "    .option(\"host\", target_host) \\\n",
    "    .option(\"username\", target_username) \\\n",
    "    .option(\"private_key_content\", ssh_key_content) \\\n",
    "    .option(\"port\", \"22\") \\\n",
    "    .option(\"path\", remote_path) \\\n",
    "    .option(\"format\", \"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Custom SFTP Data Source Write Complete\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Technology: Paramiko SSHv2 library\")\n",
    "print(f\"API: Databricks Python Data Source API\")\n",
    "print(f\"Pattern: spark.dataSource.register() + df.write.format('sftp')\")\n",
    "print(f\"Key Distribution: private_key_content option\")\n",
    "print(f\"Written: {demo_df.count()} rows to {remote_path}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31608bcd-5e44-44db-8ba3-f07cce4eb419",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. Verify Files Written to Target SFTP\n",
    "\n",
    "List files in the target SFTP directory to prove the write succeeded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94be029a-84d0-41b9-8ec7-b01d6d89536c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get SSH private key content from secrets and create temp file for connection test\n",
    "from CustomDataSource import SFTPConnectionTester\n",
    "\n",
    "ssh_key_content = dbutils.secrets.get(scope=secret_scope, key=ssh_key_secret)\n",
    "tmp_key_file = tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='_sftp_key')\n",
    "tmp_key_file.write(ssh_key_content)\n",
    "tmp_key_file.close()\n",
    "os.chmod(tmp_key_file.name, 0o600)\n",
    "\n",
    "try:\n",
    "    # Connect to target SFTP and list files\n",
    "    target_tester = SFTPConnectionTester(\n",
    "        host=target_host,\n",
    "        username=target_username,\n",
    "        private_key_path=tmp_key_file.name,\n",
    "        port=22\n",
    "    )\n",
    "\n",
    "    with target_tester as conn:\n",
    "        files = conn.list_files(\".\")\n",
    "        print(\"Files in target SFTP directory:\")\n",
    "        print(\"=\"*70)\n",
    "        for f in files:\n",
    "            print(f\"  - {f}\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Check if our file exists\n",
    "        if \"demo_customers.csv\" in files or any(\"demo_customers\" in f for f in files):\n",
    "            print(\"\\n✅ SUCCESS: demo_customers.csv file(s) found in target SFTP!\")\n",
    "        else:\n",
    "            print(\"\\n⚠️  WARNING: demo_customers.csv not found in file list\")\n",
    "            \n",
    "finally:\n",
    "    # Clean up temporary key file\n",
    "    if os.path.exists(tmp_key_file.name):\n",
    "        os.remove(tmp_key_file.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b80f194b-d727-4818-854b-732565bec259",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "SFTP structured streaming demo completed:\n",
    "- ✓ Read data from source SFTP using AutoLoader\n",
    "- ✓ Displayed data in table\n",
    "- ✓ Wrote data to target SFTP using custom data source\n",
    "- ✓ Verified files exist in target directory\n",
    "\n",
    "**Key Technologies:**\n",
    "- AutoLoader (built-in Databricks) for SFTP reads\n",
    "- Paramiko 3.4.0 + Data Source API for SFTP writes\n",
    "- Unity Catalog connections for credential management\n",
    "- Managed volumes for checkpoint storage"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "03_sftp_structured_streaming",
   "widgets": {
    "catalog_name": {
     "currentValue": "sftp_demo",
     "nuid": "d609b09d-8423-4508-9137-052deb96bb0d",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "sftp_demo",
      "label": "Catalog Name",
      "name": "catalog_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "sftp_demo",
      "label": "Catalog Name",
      "name": "catalog_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "schema_name": {
     "currentValue": "default",
     "nuid": "9c168dfb-f95c-4189-bdfe-f07e71025e08",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "default",
      "label": "Schema Name",
      "name": "schema_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "default",
      "label": "Schema Name",
      "name": "schema_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
