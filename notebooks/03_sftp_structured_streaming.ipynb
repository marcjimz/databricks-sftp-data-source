{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 03 - SFTP Structured Streaming Demo\n",
        "\n",
        "This notebook demonstrates reading and writing data with SFTP using:\n",
        "- **AutoLoader** for reading from SFTP (built-in Databricks feature)\n",
        "- **Custom SFTP Data Source** for writing to SFTP (Paramiko + Data Source API)\n",
        "\n",
        "**Prerequisites:**\n",
        "- Run notebook `01_infrastructure_setup.ipynb` first\n",
        "- Run notebook `02_uc_connection_setup.ipynb` second\n",
        "\n",
        "This notebook will:\n",
        "1. Read data from source SFTP using AutoLoader\n",
        "2. Write data to target SFTP using custom data source\n",
        "3. List files in target directory to verify"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Install Package and Load Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install package from source (editable mode)\n",
        "%pip install -q -e ..\n",
        "dbutils.library.restartPython()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create widgets for catalog and schema configuration\n",
        "dbutils.widgets.text(\"catalog_name\", \"sftp_demo\", \"Catalog Name\")\n",
        "dbutils.widgets.text(\"schema_name\", \"default\", \"Schema Name\")\n",
        "\n",
        "# Get widget values\n",
        "CATALOG_NAME = dbutils.widgets.get(\"catalog_name\")\n",
        "SCHEMA_NAME = dbutils.widgets.get(\"schema_name\")\n",
        "\n",
        "print(f\"Catalog: {CATALOG_NAME}\")\n",
        "print(f\"Schema: {SCHEMA_NAME}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load configuration from catalog\n",
        "config_df = spark.table(f\"{CATALOG_NAME}.config.connection_params\")\n",
        "config_dict = {row.key: row.value for row in config_df.collect()}\n",
        "\n",
        "# Get configuration values\n",
        "source_host = config_dict[\"source_host\"]\n",
        "source_username = config_dict[\"source_username\"]\n",
        "target_host = config_dict[\"target_host\"]\n",
        "target_username = config_dict[\"target_username\"]\n",
        "secret_scope = config_dict[\"secret_scope\"]\n",
        "ssh_key_secret = config_dict[\"ssh_key_secret\"]\n",
        "\n",
        "print(\"✓ Configuration loaded successfully\")\n",
        "print(f\"  Source: {source_username}@{source_host}\")\n",
        "print(f\"  Target: {target_username}@{target_host}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Read from SFTP using AutoLoader\n",
        "\n",
        "AutoLoader automatically finds the Unity Catalog connection based on the host in the SFTP URI.\n",
        "\n",
        "This uses:\n",
        "- **Checkpoint location**: Managed volume (`/Volumes/{catalog}/{schema}/_checkpoints`)\n",
        "- **Schema inference**: AutoLoader automatically infers and evolves the schema\n",
        "- **Serverless trigger**: `availableNow=True` for single micro-batch processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test reading customers.csv from source SFTP using AutoLoader\n",
        "source_sftp_uri = f\"sftp://{source_username}@{source_host}:22/customers.csv\"\n",
        "\n",
        "# Use managed volume for checkpoint location\n",
        "checkpoint_location = f\"/Volumes/{CATALOG_NAME}/{SCHEMA_NAME}/_checkpoints/customers\"\n",
        "schema_location = f\"/Volumes/{CATALOG_NAME}/{SCHEMA_NAME}/_checkpoints/customers_schema\"\n",
        "\n",
        "print(f\"AutoLoader configuration:\")\n",
        "print(f\"  Source URI: {source_sftp_uri}\")\n",
        "print(f\"  Checkpoint: {checkpoint_location}\")\n",
        "print(f\"  Schema: {schema_location}\\n\")\n",
        "\n",
        "customers_df = (\n",
        "    spark.readStream\n",
        "    .format(\"cloudFiles\")\n",
        "    .option(\"cloudFiles.format\", \"csv\")\n",
        "    .option(\"cloudFiles.schemaLocation\", schema_location)\n",
        "    .option(\"header\", \"true\")\n",
        "    .load(source_sftp_uri)\n",
        ")\n",
        "\n",
        "# Display schema\n",
        "print(\"Schema:\")\n",
        "customers_df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Write streaming data to table using checkpoint location in managed volume\n",
        "table_name = f\"{CATALOG_NAME}.{SCHEMA_NAME}.test_customers\"\n",
        "\n",
        "print(f\"Writing to table: {table_name}\")\n",
        "print(f\"Checkpoint location: {checkpoint_location}\\n\")\n",
        "\n",
        "query = (\n",
        "    customers_df.writeStream\n",
        "    .trigger(availableNow=True)\n",
        "    .option(\"checkpointLocation\", checkpoint_location)\n",
        "    .toTable(table_name)\n",
        ")\n",
        "\n",
        "# Wait for the micro-batch to complete\n",
        "query.awaitTermination()\n",
        "\n",
        "print(\"✅ All data processed!\")\n",
        "print(f\"\\nData read from SFTP:\")\n",
        "spark.sql(f\"SELECT * FROM {table_name}\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Write to SFTP using Custom Data Source\n",
        "\n",
        "The custom SFTP Data Source uses:\n",
        "- **Databricks Python Data Source API** - Native Spark integration\n",
        "- **Paramiko 3.4.0** - Secure SSH/SFTP protocol implementation\n",
        "- **Distributed writes** - Each Spark executor creates its own temp key file\n",
        "\n",
        "This enables writing using the standard Spark API: `df.write.format(\"sftp\")`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tempfile\n",
        "import os\n",
        "\n",
        "# Import custom SFTP Data Source\n",
        "from CustomDataSource import SFTPDataSource\n",
        "\n",
        "# Register the SFTP data source with Spark\n",
        "spark.dataSource.register(SFTPDataSource)\n",
        "\n",
        "print(\"✓ SFTP data source registered with Spark\")\n",
        "print(f\"  Format name: 'sftp'\")\n",
        "print(f\"  Usage: df.write.format('sftp').option(...).save()\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create demo DataFrame to write to target SFTP\n",
        "from datetime import datetime\n",
        "\n",
        "demo_data = [\n",
        "    (1, \"Demo Customer 1\", \"demo1@example.com\", \"USA\", datetime.now().strftime(\"%Y-%m-%d\")),\n",
        "    (2, \"Demo Customer 2\", \"demo2@example.com\", \"UK\", datetime.now().strftime(\"%Y-%m-%d\")),\n",
        "    (3, \"Demo Customer 3\", \"demo3@example.com\", \"Canada\", datetime.now().strftime(\"%Y-%m-%d\"))\n",
        "]\n",
        "\n",
        "demo_df = spark.createDataFrame(demo_data, [\"customer_id\", \"name\", \"email\", \"country\", \"signup_date\"])\n",
        "\n",
        "print(\"Demo DataFrame created:\")\n",
        "demo_df.show()\n",
        "\n",
        "print(f\"\\n✓ Created demo DataFrame with {demo_df.count()} rows\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Write demo data to target SFTP using Databricks Python Data Source API\n",
        "# The SSH key content is passed as an option - each Spark executor creates its own temp file\n",
        "\n",
        "# Get SSH private key content from secrets\n",
        "ssh_key_content = dbutils.secrets.get(scope=secret_scope, key=ssh_key_secret)\n",
        "\n",
        "remote_path = \"/demo_customers.csv\"\n",
        "\n",
        "print(f\"Writing demo DataFrame to SFTP using Spark DataSource API\")\n",
        "print(f\"Target: {target_username}@{target_host}{remote_path}\")\n",
        "print(f\"Technology: Paramiko SSHv2 library (version 3.4.0)\")\n",
        "print(f\"Distribution: Each Spark executor creates local temp key file\\n\")\n",
        "\n",
        "# Write using Spark DataSource API - passes key content, not path\n",
        "# Each executor will create its own temporary key file from the content\n",
        "demo_df.write \\\n",
        "    .format(\"sftp\") \\\n",
        "    .option(\"host\", target_host) \\\n",
        "    .option(\"username\", target_username) \\\n",
        "    .option(\"private_key_content\", ssh_key_content) \\\n",
        "    .option(\"port\", \"22\") \\\n",
        "    .option(\"path\", remote_path) \\\n",
        "    .option(\"format\", \"csv\") \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .save()\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Custom SFTP Data Source Write Complete\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Technology: Paramiko SSHv2 library\")\n",
        "print(f\"API: Databricks Python Data Source API\")\n",
        "print(f\"Pattern: spark.dataSource.register() + df.write.format('sftp')\")\n",
        "print(f\"Key Distribution: private_key_content option (not file path)\")\n",
        "print(f\"Written: {demo_df.count()} rows to {remote_path}\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Verify Files Written to Target SFTP\n",
        "\n",
        "List files in the target SFTP directory to prove the write succeeded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get SSH private key content from secrets and create temp file for connection test\n",
        "from CustomDataSource import SFTPConnectionTester\n",
        "\n",
        "ssh_key_content = dbutils.secrets.get(scope=secret_scope, key=ssh_key_secret)\n",
        "tmp_key_file = tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='_sftp_key')\n",
        "tmp_key_file.write(ssh_key_content)\n",
        "tmp_key_file.close()\n",
        "os.chmod(tmp_key_file.name, 0o600)\n",
        "\n",
        "try:\n",
        "    # Connect to target SFTP and list files\n",
        "    target_tester = SFTPConnectionTester(\n",
        "        host=target_host,\n",
        "        username=target_username,\n",
        "        private_key_path=tmp_key_file.name,\n",
        "        port=22\n",
        "    )\n",
        "\n",
        "    with target_tester as conn:\n",
        "        files = conn.list_files(\".\")\n",
        "        print(\"Files in target SFTP directory:\")\n",
        "        print(\"=\"*70)\n",
        "        for f in files:\n",
        "            print(f\"  - {f}\")\n",
        "        print(\"=\"*70)\n",
        "        \n",
        "        # Check if our file exists\n",
        "        if \"demo_customers.csv\" in files or any(\"demo_customers\" in f for f in files):\n",
        "            print(\"\\n✅ SUCCESS: demo_customers.csv file(s) found in target SFTP!\")\n",
        "        else:\n",
        "            print(\"\\n⚠️  WARNING: demo_customers.csv not found in file list\")\n",
        "            \n",
        "finally:\n",
        "    # Clean up temporary key file\n",
        "    if os.path.exists(tmp_key_file.name):\n",
        "        os.remove(tmp_key_file.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "SFTP structured streaming demo completed:\n",
        "- ✓ Read data from source SFTP using AutoLoader\n",
        "- ✓ Displayed data in table\n",
        "- ✓ Wrote data to target SFTP using custom data source\n",
        "- ✓ Verified files exist in target directory\n",
        "\n",
        "**Key Technologies:**\n",
        "- AutoLoader (built-in Databricks) for SFTP reads\n",
        "- Paramiko 3.4.0 + Data Source API for SFTP writes\n",
        "- Unity Catalog connections for credential management\n",
        "- Managed volumes for checkpoint storage"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
